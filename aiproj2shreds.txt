package robosim.ai;

import learning.core.Histogram;
import robosim.core.*;
import robosim.reinforcement.QTable;
import robosim.core.Simulator;
import robosim.core.Polar;

import javax.xml.crypto.dsig.keyinfo.KeyValue;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.Optional;

public class QBot implements Controller {
    private int direction = 0;
    private double turns = 1;
    private LinkedHashMap<Integer,Integer> seen = new LinkedHashMap<>();
    private QTable qtable = new QTable(64,4,0,10,0.9,0.4);
    @Override
    public void control(Simulator sim) {
        if (qtable.getLastAction() == 0) { direction++; }
        if (qtable.getLastAction() == 1) { direction--; }
        direction += 8; direction %= 8;
        int state = sim.getSector();
        double time = sim.getTotalMoves()+1;
        boolean colliding = sim.isColliding();
        int loc = sim.getLoc();
        boolean nu = !seen.containsKey(loc);
        if (nu) { seen.put(loc,1); }
        else { int v = seen.get(loc); seen.put(loc,v+1); }
        double timesivebeenhere = seen.get(loc);
        double dirt = sim.getDirt()+1;
        double reward =  (double )(seen.size() - timesivebeenhere) / turns;
        if (colliding) {reward = -1;}
        int action = qtable.senseActLearn(state,reward);
        if      (action == 0)             { Action.LEFT.applyTo(sim);     turns++; }
        else if (action == 1)             { Action.RIGHT.applyTo(sim);    turns++; }
        else if (action == 2)             { Action.BACKWARD.applyTo(sim); turns=1;}
        else if (action == 3)             { Action.FORWARD.applyTo(sim);  turns=1;}
        else { for (int i=0;i<action;i++) { Action.FORWARD.applyTo(sim);  } }
    }
}

package robosim.ai;

import learning.core.Histogram;
import robosim.core.*;
import robosim.reinforcement.QTable;
import robosim.core.Simulator;
import java.util.LinkedHashMap;

public class QBot implements Controller {
    private QTable qtable = new QTable(100,2,0,10,0.9,0.4);
    @Override
    public void control(Simulator sim) {
        double wh = 1.0;
        double fcp = 1.0;
        if (sim.wasHit()) {wh = 0.0;}
        if (sim.findClosestProblem() < 30) {
            fcp = sim.findClosestProblem() / 30;
        }
        double reward = wh*fcp;
        int action = qtable.senseActLearn(state,reward);
        if      (action == 0)             { Action.LEFT.applyTo(sim);     forwards=1; turns++; }
        else if (action == 1)             { Action.RIGHT.applyTo(sim);    forwards=1; turns++; }
        else if (action == 2)             { Action.BACKWARD.applyTo(sim); forwards++; turns=1;}
        else if (action == 3)             { Action.FORWARD.applyTo(sim);  forwards++; turns=1;}
    }
}
package robosim.ai;

import learning.core.Histogram;
import robosim.core.*;
import robosim.reinforcement.QTable;
import robosim.core.Simulator;
import java.util.LinkedHashMap;

public class QBot implements Controller {
    private QTable qtable = new QTable(4,3,0,8,0.05,0.7);
    private int direction = 0;
    private LinkedHashMap<Integer,Double> seen = new LinkedHashMap<>();
    private double forwards = 2.0;
    @Override
    public void control(Simulator sim) {
        if (qtable.getLastAction() == 1) { direction++; forwards--; }
        if (qtable.getLastAction() == 2) { direction--; forwards--; }
        else { forwards+=Math.abs(forwards); }
        double time = sim.getTotalMoves()+1;
        double fs = sim.getForwardMoves()+1;
        if (sim.findClosestProblem() < 30) {forwards=sim.findClosestProblem()/15.0;}
        direction += 8; direction %= 8;
        double wh = 1.0;
        if (sim.wasHit()) {wh = 0.0; forwards=-1;}
        double reward = wh*fs/time;
        int action = qtable.senseActLearn(direction % 4,reward);
        if (action == 1) { Action.LEFT.applyTo(sim);  }
        if (action == 2) { Action.RIGHT.applyTo(sim);  }
        else             { Action.FORWARD.applyTo(sim); }
    }
}
package robosim.ai;

import learning.core.Histogram;
import robosim.core.*;
import robosim.reinforcement.QTable;
import robosim.core.Simulator;
import java.util.LinkedHashMap;

public class QBot implements Controller {
    private QTable qtable = new QTable(4,3,0,60,0.05,0.7);
    private int direction = 0;
    private LinkedHashMap<Integer,Double> seen = new LinkedHashMap<>();
    private double forwards = 1.0;
    @Override
    public void control(Simulator sim) {
        double reward = forwards;
        forwards++;
        if (qtable.getLastAction() > 0) {
            if (sim.findClosestProblem() < 30) {
                reward = 1;
            }
            else {
                reward = -1;
            }
        }
        if (sim.wasHit()) {reward = -2; forwards=1.0;}
        direction += 8; direction %= 8;
        double wh = 1.0;
        int action = qtable.senseActLearn(direction % 4,reward);
        if (action == 1) { Action.LEFT.applyTo(sim);  }
        if (action == 2) { Action.RIGHT.applyTo(sim);  }
        else             { Action.FORWARD.applyTo(sim); }
    }
}
